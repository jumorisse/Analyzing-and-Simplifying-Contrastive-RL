{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of Actor Behaviour\n",
        "\n",
        "Eysenbach et al. (2023) argue that their contrastive representation learning leads to representations that encode actionable distances. Further, they say the distance between a state-action representation and a goal representaiton corresponds to the probability of reaching that goal when performing the respective action in the respective state. If that is true, the trained encoders could be used for successful control without the need for an actor network. Instead, the representations for all current state-action pairs could be evaluated and the one closest to the goal representation could be chosen greedily.\n",
        "\n",
        "In this experiment's first part, I want to investigate how the actor network behaves. Does it behave in accordance to this greedy selection or does it employ a more complex strategy?"
      ],
      "metadata": {
        "id": "8HtJlURXBPC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Encoders & Actor\n",
        "Again, we first need to load the encoders and the actor trained during the contrastive RL."
      ],
      "metadata": {
        "id": "8bzryaY0APLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf5jUadkAOBX"
      },
      "outputs": [],
      "source": [
        "import experiment_utils as utils\n",
        "\n",
        "env = \"Spiral11x11\"\n",
        "sa_encoder, g_encoder, actor = utils.load_trained_networks(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Environment and States\n",
        "Then, we collect all the states of the environment."
      ],
      "metadata": {
        "id": "gPz6om5dAiJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = utils.get_all_env_states(env)"
      ],
      "metadata": {
        "id": "zavQRopyBWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Tasks (start states and goals)\n",
        "To evaluate how the greedy selection strategy compares to the actor, we need to sample a few tasks.\n",
        "\n",
        "Each task consists of a start state and a goal state."
      ],
      "metadata": {
        "id": "SxQ-cFcyAsxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = utils.sample_tasks(states, n=1000)"
      ],
      "metadata": {
        "id": "6Kpi5h9WBWzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recording Actor Behaviour on Sampled Tasks\n",
        "Now we record the actor's performance on the sampled tasks. We record the following metrics as averages across all tasks:\n",
        "- success rate\n",
        "- number of steps needed\n",
        "- time needed"
      ],
      "metadata": {
        "id": "EDt2eCzOA9I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actor_trajectories, actor_metrics = utils.evaluate_actor(actor, tasks)"
      ],
      "metadata": {
        "id": "_krDFK13BXYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Greedy Behaviour on Sampled Tasks\n",
        "Now we don't use the actor but instead greedily select the next action based on which state-action representation is closest to the goal representation.\n",
        "\n",
        "The recorded metrics are the same."
      ],
      "metadata": {
        "id": "eSCe_MV3BYf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_trajectories, greedy_metrics = utils.evaluate_encoder_selection(\"greedy\", tasks)"
      ],
      "metadata": {
        "id": "MFpq7fp1BeDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Actor & Greedy Behaviour\n",
        "We now compare the obtained metrics and also compute the degree of agreement between actor selection and greedy selection, i.e. the percentage of identical decisions in identical situations (same state in same task)."
      ],
      "metadata": {
        "id": "hKngkqaOBeXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "utils.compare_metrics((\"actor\", \"greedy\"), (actor_metrics, greedy_metrics))\n",
        "agreement = utils.compute_agreement(actor_trajectories, greedy_trajectories)\n"
      ],
      "metadata": {
        "id": "PHHERgU5Bhkp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "collapsed": true,
        "outputId": "8f6a60de-7f44-4c9b-8d00-751990d211a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'utils' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-337e772a82a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomparison\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"actor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"greedy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactor_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Going Beyond Greedy Selection\n",
        "\n",
        "The previous experiment can have two possible outcomes:\n",
        "1. The actor behaves greedily, always choosing the action for which the state-action representation is closest to the goal representation.\n",
        "2. The actor does not behave greedily.\n",
        "\n",
        "The first result would support the claim that the representations' distance does actually correspond to the probability of reaching the goal in the future. In this case, it could be interesting to see whether explicitly using selection strategies with a further decision horizon can improve beyond greedy search (which would challenge the probability assumption).\n",
        "\n",
        "The second result would challenge the probability assumption and show that the actor learns a more complex strategy than just greedy selection. In this case, it would be interesting to see what other strategy better models the actor strategy.\n",
        "\n",
        "In both cases, we need a number of more complex selection strategies that are better at taking into account effects in the future. Possible candidates are:\n",
        "- Monte Carlo Tree Search\n",
        "- Limited Breadth Firt Search\n",
        "- Limited Depth First Search\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7KhOgEO3BGzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Encoder & Actor"
      ],
      "metadata": {
        "id": "AtcyYDA4Dlf5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynaqVUoODk-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Algorithms"
      ],
      "metadata": {
        "id": "WchWsyTqDriF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcsZfZu-DvKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Environment & States"
      ],
      "metadata": {
        "id": "ylxuSDdaDvd-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hSy4jgOEDyiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Tasks (start states and goals)"
      ],
      "metadata": {
        "id": "aMCcvt0QDy0U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nmiz_hgND2K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recording Actor & Algorithm Behaviour on Sampled Tasks"
      ],
      "metadata": {
        "id": "3oeniXj0D2b9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zp2c4PulD8oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Performance of Actor & Algorithms"
      ],
      "metadata": {
        "id": "n0NoEAfGD88L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsY4_AwEEEUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}